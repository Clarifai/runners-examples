import os
import sys

sys.path.append(os.path.dirname(__file__))
from openai_server_starter import OpenAI_APIServer

##################
import json
from typing import Dict, Iterator, List, Union

from clarifai.runners.models.model_builder import ModelBuilder
from clarifai.runners.models.openai_class import OpenAIModelClass
from clarifai.utils.logging import logger
from clarifai.runners.utils.data_types import (Image, Video, Audio)
from clarifai.runners.utils.data_utils import Param
from clarifai.runners.utils.openai_convertor import build_openai_messages

from openai import OpenAI
from openai.types.chat import (ChatCompletionChunk, ChatCompletion)
from openai.resources.completions import Stream as OpenAIStream


class ModelBadRequestError(Exception):
  pass

def build_messages(
  prompt: str = "", 
  system_prompt: str = "", 
  images: List[Image] = [],
  audios: List[Audio] = [],
  videos: List[Video] = [],
  chat_history: List[Dict] = [],
  **kwargs
) -> List[Dict]:
  """Construct OpenAI-compatible messages from input components."""
    
  openai_messages = build_openai_messages(
    prompt=prompt,
    images=images,
    audios=audios,
    videos=videos,
    messages=chat_history
  )
  
  if not chat_history and system_prompt:
    openai_messages = [{
        "role": "system",
        "content": system_prompt
    }] + openai_messages

  return openai_messages


class MyRunner(OpenAIModelClass):
  """
  A custom runner that integrates with the Clarifai platform and uses Server inference
  to process inputs, including text and images.
  """
  client = True
  model = True
  def load_model(self):
    """Load the model here and start the  server."""
    config_path = os.path.dirname(os.path.dirname(__file__))

    # server args were generated by `upload` module
    server_args = {'backend': 'turbomind', 'cache_max_entry_count': 0.95, 
                   'tensor_parallel_size': 1, 'max_prefill_token_num': 8192, 'dtype': 'auto', 
                   'quantization_format': None, 'quant_policy': 0, 'chat_template': 'llama3_2', 'max_batch_size': 16, 
                   'device': 'cuda', 'server_name': '0.0.0.0', 'server_port': 23333, 
                   'additional_list_args': ['--tool-call-parser', 'llama3'], 'checkpoints': 'runtime'
                   }

    builder = ModelBuilder(config_path, download_validation_only=True)
    
    stage = server_args.get("checkpoints")
    if stage in ["build", "runtime"]:
      #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")
      checkpoints = builder.download_checkpoints(stage=stage)
      server_args.update({"checkpoints": checkpoints})

    # Start server
    # This line were generated by `upload` module
    self.server = OpenAI_APIServer.from_lmdeploy_backend(**server_args)

    self.client = OpenAI(
            api_key="notset",
            base_url=MyRunner.make_api_url(self.server.host, self.server.port))
    self.model = self._get_model_id()
  
  # --------------------- Helper methods -------------------- #
  
  @staticmethod
  def make_api_url(host: str, port: int, version: str = "v1") -> str:
    return f"http://{host}:{port}/{version}"
  
  def _get_model_id(self):
    try:
      return self.client.models.list().data[0].id
    except Exception as e:
      raise ConnectionError("Failed to retrieve model ID from API") from e

  def _cl_custom_chat(
    self,
    prompt: str = "",
    system_prompt: str = "",
    images: List[Image] = [],
    audios: List[Audio] = [],
    videos: List[Video] = [],
    chat_history: List[Dict] = [],
    max_tokens: int = 512,
    temperature: float = 0.7,
    top_p: float = 0.8,
    stream=False,
    **completion_kwargs
  ) -> dict:
    """Process request through OpenAI API."""
    openai_messages = build_messages(
        prompt, system_prompt, images, audios, videos, chat_history)
    
    if stream:
      # Force to use usage
      stream_options = completion_kwargs.pop("stream_options", {})
      stream_options.update({"include_usage": True})
      completion_kwargs["stream_options"] = stream_options
    
    if "tool_choice" in completion_kwargs and completion_kwargs.get("tool_choice") is None:
      completion_kwargs.pop("tool_choice", None)
    
    logger.info(f"additional completion_kwargs = {completion_kwargs}")
    
    response = self.client.chat.completions.create(
        model=self.model,
        messages=openai_messages,
        temperature=temperature,
        max_tokens=max_tokens,
        top_p=top_p,
        stream=stream,
        **completion_kwargs
      )

    return response
  
  # --------------------- Playground UI methods ------------------- # 
  
  @OpenAIModelClass.method
  def predict(
    self,
    prompt: str = "",
    images: List[Image] = [],
    audios: List[Audio] = [],
    videos: List[Video] = [],
    chat_history: List[Dict] = [],
    audio: Audio = None,
    video: Video = None,
    image: Image = None,
    tools: List[dict] = None,
    tool_choice: str = None,
    system_prompt: str = Param(default="", description="The system-level prompt used to define the assistant's behavior."),
    max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
    temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response.", ),
    top_p: float = Param(default=0.9, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", ),
  )-> str:
    """Method to call from UI
    """
    logger.info(f"Input: {prompt=}, {image=}")
    response = self._cl_custom_chat(
      prompt=prompt,
      system_prompt=system_prompt,
      images=[image] if image and (image.bytes or image.url) else images,
      audios=[audio] if audio and (audio.bytes or audio.url) else audios,
      videos=[video] if video and (video.bytes or video.url) else videos,
      chat_history=chat_history,
      max_tokens=max_tokens,
      temperature=temperature,
      top_p=top_p,
      stream=False,
      tool_choice=tool_choice,
      tools=tools,
    )
    
    # If the response contains tool calls, return as a string
    if response.choices and response.choices[0].message.tool_calls:
        tool_calls = response.choices[0].message.tool_calls
        tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)
        return tool_calls_json
    # Otherwise, return the content of the first choice
    else:
        return response.choices[0].message.content
  
  @OpenAIModelClass.method
  def generate(
    self,
    prompt: str = "",
    images: List[Image] = [],
    audios: List[Audio] = [],
    videos: List[Video] = [],
    chat_history: List[Dict] = [],
    audio: Audio = None,
    video: Video = None,
    image: Image = None,
    tools: List[dict] = None,
    tool_choice: str = None,
    system_prompt: str = Param(default="", description="The system-level prompt used to define the assistant's behavior."),
    max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
    temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response.", ),
    top_p: float = Param(default=0.9, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", ),
  ) -> Iterator[str]:
    """Method to call generate from UI
    """
    logger.info(f"Input: {prompt=}, {image=}")
    stream_completion = self._cl_custom_chat(
        prompt=prompt,
        system_prompt=system_prompt,
        images=[image] if image and (image.bytes or image.url) else images,
        audios=[audio] if audio and (audio.bytes or audio.url) else audios,
        videos=[video] if video and (video.bytes or video.url) else videos,
        chat_history=chat_history,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True,
        tool_choice=tool_choice,
        tools=tools,
    )

    for chunk in stream_completion:
      # If the response contains tool calls, return the first one as a string
      if chunk.choices:
        if chunk.choices[0].delta.tool_calls:
          tool_calls = chunk.choices[0].delta.tool_calls
          tool_calls_json = [tc.to_dict() for tc in tool_calls]
          # Convert to JSON string
          json_string = json.dumps(tool_calls_json, indent=2)
          # Yield the JSON string
          yield json_string
        else:
          yield str(chunk.choices[0].delta.content)
  
  
  # ---------------- Test locally ------------------- #
  
  
  def test(self):
    
    build_modalities_kwargs = {}
    prompt = "Hi there, what's my name?"
    build_modalities_kwargs.update(dict(prompt=prompt))
  
    # dummy history
    chat_history = [
      dict(content="You're an assistant.", role="system"),
      dict(content="Hi, my name is Clarifai", role="user"),
    ]
    # Test predict
    logger.info("# 1: Test 'predict'.\n")
    print(self.predict(**build_modalities_kwargs, max_tokens=512, temperature=0.9, top_p=0.9, system_prompt="You are an Orange Cat."))
    print("---"*5)
    
    logger.info("# 1.2: Test 'predict' History.\n")
    print(self.predict(**build_modalities_kwargs, chat_history=chat_history, max_tokens=512, temperature=0.9, top_p=0.9, system_prompt="You are an Orange Cat."))
    print("---"*5)
    
    # Test generate
    logger.info("# 2: Test 'generate'.\n")
    for each in self.generate(**build_modalities_kwargs, max_tokens=512, temperature=0.9, top_p=0.9, system_prompt=""):
      print(each, flush=True, end='')
    print("---"*5)
    
    logger.info("# 2.2: Test 'generate' History.\n")
    for each in self.generate(**build_modalities_kwargs, chat_history=chat_history, max_tokens=512, temperature=0.9, top_p=0.9, system_prompt=""):
      print(each, flush=True, end='')
    print("---"*5)
    
    
    logger.info("# 3: Test openai_transport")
    
    openai_request = {
      "messages": [
        {
          "role": "user",
          "content": "Hi, can you help me plan a trip to Japan?"
        },
        {
          "role": "assistant",
          "content": "Sure! When are you planning to go and what cities are you interested in visiting?"
        },
        {
          "role": "user",
          "content": "I’ll be there in October. I’d like to visit Tokyo, Kyoto, and Osaka."
        },
        {
          "role": "assistant",
          "content": "Great! October is a beautiful time in Japan. Would you prefer a cultural tour, food-focused trip, or something else?"
        },
        {
          "role": "user",
          "content": "I'm mostly interested in cultural sites and some local cuisine."
        }
      ],
      "temperature": 0.7,
      "top_p": 1.0,
      "n": 1,
      "max_tokens": 512,
    }
    #logger.info(f"{openai_request=}")
    string_request = json.dumps(openai_request)
    print(self.openai_transport(string_request))
    print("---"*5)
    
    logger.info("# 4: Test openai_stream_transport. Print out 5 chunks")
    openai_request["stream"] = True
    string_request = json.dumps(openai_request)
    iter_resp = self.openai_stream_transport(string_request)
    c = 0
    for each in iter_resp:
      print(each)
      c += 1
      if c > 5:
        break
    
    print("---"*5)