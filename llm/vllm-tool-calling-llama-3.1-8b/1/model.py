import os
import sys

sys.path.append(os.path.dirname(__file__))
from typing import List, Iterator

from clarifai.runners.models.model_builder import ModelBuilder
from clarifai.runners.models.model_class import ModelClass
from openai import OpenAI
from openai_client_wrapper import OpenAIWrapper
from openai_server_starter import OpenAI_APIServer


class LlamaToolCallingModel(ModelClass):
  """
  A custom runner that integrates with the Clarifai platform and uses Server inference
  to process inputs, including text and images.
  """

  def load_model(self):
    """Load the model here and start the  server."""
    os.path.join(os.path.dirname(__file__))
    # This is the path to the chat template file and you can get this chat template from vLLM repo(https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.1_json.jinja)
    chat_template = os.path.join(os.path.dirname(__file__), "tool_chat_template_llama3.1_json.jinja")

    server_args = {
        'max_model_len': 2048,
        'gpu_memory_utilization': 0.8,
        'dtype': 'auto',
        'task': 'auto',
        'kv_cache_dtype': 'auto',
        'tensor_parallel_size': 1,
        'quantization': None,
        'chat_template': chat_template,
        'cpu_offload_gb': 0.0,
        'port': 23333,
        'host': 'localhost',
        "enable_auto_tool_choice": True,
        'tool_call_parser': "llama3_json",
        'checkpoints': "runtime"
    }

    # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path
    stage = server_args.get("checkpoints")
    if stage in ["build", "runtime"]:
      #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")
      config_path = os.path.dirname(os.path.dirname(__file__))
      builder = ModelBuilder(config_path, download_validation_only=True)
      checkpoints = builder.download_checkpoints(stage=stage)
      server_args.update({"checkpoints": checkpoints})

    if server_args.get("additional_list_args") == ['']:
      server_args.pop("additional_list_args")

    # Start server
    # This line were generated by `upload` module
    self.server = OpenAI_APIServer.from_vllm_backend(**server_args)
    # Create client
    self.client = OpenAIWrapper(
        client=OpenAI(
            api_key="notset",
            base_url=OpenAIWrapper.make_api_url(self.server.host, self.server.port)),)

  @ModelClass.method
  def predict(self,
              prompt: str,
              chat_history: List[dict] = None,
              tools: List[dict] = None,
              max_tokens: int = 512,
              temperature: float = 0.7,
              top_p: float = 0.8) -> str:
    """
    Predict the response for the given prompt and chat history using the model and tools.
    """
    response= self.client.chat(
        prompt=prompt,
        messages=chat_history,
        tools=tools,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p)
    if response.choices[0] and response.choices[0].message.tool_calls:
      import json
      # If the response contains tool calls, return as a string

      tool_calls = response.choices[0].message.tool_calls
      tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)
      return tool_calls_json
    else:
      # Otherwise, return the content of the first choice
      return response.choices[0].message.content
    

  @ModelClass.method
  def generate(self,
               prompt: str,
               chat_history: List[dict] = None,
               tools: List[dict] = None,
               max_tokens: int = 512,
               temperature: float = 0.7,
               top_p: float = 0.8) -> Iterator[str]:
    """Stream generated text tokens from a prompt + optional chat history and tools."""
    for chunk in self.client.chat(
        prompt=prompt,
        messages=chat_history,
        tools=tools,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True):
      if chunk.choices:
        if chunk.choices[0].delta.tool_calls:
          # If the response contains tool calls, return the first one as a string
          import json
          tool_calls = chunk.choices[0].delta.tool_calls
          tool_calls_json = [tc.to_dict() for tc in tool_calls]
          # Convert to JSON string
          json_string = json.dumps(tool_calls_json, indent=2)
          # Yield the JSON string
          yield json_string
        else:
          # Otherwise, return the content of the first choice
          text = (chunk.choices[0].delta.content
                  if (chunk and chunk.choices[0].delta.content) is not None else '')
          yield text

  @ModelClass.method
  def chat(self,
           messages: List[dict],
           tools: List[dict] = None,
           max_tokens: int = 512,
           temperature: float = 0.7,
           top_p: float = 0.8) -> Iterator[dict]:
    """Chat with the model."""
    for chunk in self.client.chat(
        messages=messages,
        tools=tools,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True):
      yield chunk.to_dict()
