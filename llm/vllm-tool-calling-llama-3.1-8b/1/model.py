import os
import sys

sys.path.append(os.path.dirname(__file__))
from typing import List, Iterator

from clarifai.runners.models.model_builder import ModelBuilder
from clarifai.runners.models.model_class import ModelClass
from clarifai.runners.utils.data_utils import Param
from openai import OpenAI
from openai_client_wrapper import OpenAIWrapper
from openai_server_starter import OpenAI_APIServer


class LlamaToolCallingModel(ModelClass):
  """
  A custom runner that integrates with the Clarifai platform and uses Server inference
  to process inputs, including text and images.
  """

  def load_model(self):
    """Load the model here and start the  server."""
    os.path.join(os.path.dirname(__file__))
    # This is the path to the chat template file and you can get this chat template from vLLM repo(https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.1_json.jinja)
    chat_template = os.path.join(os.path.dirname(__file__), "tool_chat_template_llama3.1_json.jinja")

    server_args = {
        'max_model_len': 2048,
        'gpu_memory_utilization': 0.8,
        'dtype': 'auto',
        'task': 'auto',
        'kv_cache_dtype': 'auto',
        'tensor_parallel_size': 1,
        'quantization': None,
        'chat_template': chat_template,
        'cpu_offload_gb': 0.0,
        'port': 23333,
        'host': 'localhost',
        "enable_auto_tool_choice": True,
        'tool_call_parser': "llama3_json",
        'checkpoints': "runtime"
    }

    # if checkpoints == "checkpoints" => assign to checkpoints var aka local checkpoints path
    stage = server_args.get("checkpoints")
    if stage in ["build", "runtime"]:
      #checkpoints = os.path.join(os.path.dirname(__file__), "checkpoints")
      config_path = os.path.dirname(os.path.dirname(__file__))
      builder = ModelBuilder(config_path, download_validation_only=True)
      checkpoints = builder.download_checkpoints(stage=stage)
      server_args.update({"checkpoints": checkpoints})

    if server_args.get("additional_list_args") == ['']:
      server_args.pop("additional_list_args")

    # Start server
    # This line were generated by `upload` module
    self.server = OpenAI_APIServer.from_vllm_backend(**server_args)
    # Create client
    self.client = OpenAIWrapper(
        client=OpenAI(
            api_key="notset",
            base_url=OpenAIWrapper.make_api_url(self.server.host, self.server.port)),)

  @ModelClass.method
  def predict(self,
              prompt: str,
              chat_history: List[dict] = None,
              tools: List[dict] = None,
              tool_choice: str = Param(default=None, description="The tool choice for the model. If set to 'auto', Controls which (if any) tool is called by the model.",),
              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),
              top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )
              ) -> str:
    """
    Predict the response for the given prompt and chat history using the model and tools.
    """
    if tools is None and tool_choice is None:
      tool_choice = "none"
    elif tools is not None and tool_choice is None:
      tool_choice = "auto"
      
    response= self.client.chat(
        prompt=prompt,
        messages=chat_history,
        tools=tools,
        tool_choice=tool_choice,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p)
    if response.choices[0] and response.choices[0].message.tool_calls:
      import json
      # If the response contains tool calls, return as a string

      tool_calls = response.choices[0].message.tool_calls
      tool_calls_json = json.dumps([tc.to_dict() for tc in tool_calls], indent=2)
      return tool_calls_json
    else:
      # Otherwise, return the content of the first choice
      return response.choices[0].message.content
    

  @ModelClass.method
  def generate(self,
               prompt: str,
               chat_history: List[dict] = None,
               tools: List[dict] = None,
              tool_choice: str = Param(default=None, description="The tool choice for the model. If set to 'auto', Controls which (if any) tool is called by the model.",),
              max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
              temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),
              top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )
               ) -> Iterator[str]:
    """Stream generated text tokens from a prompt + optional chat history and tools."""
    if tools is None and tool_choice is None:
        tool_choice = "none"
    elif tools is not None and tool_choice is None:
        tool_choice = "auto"
    for chunk in self.client.chat(
        prompt=prompt,
        messages=chat_history,
        tools=tools,
        tool_choice=tool_choice,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True):
      if chunk.choices:
        if chunk.choices[0].delta.tool_calls:
          # If the response contains tool calls, return the first one as a string
          import json
          tool_calls = chunk.choices[0].delta.tool_calls
          tool_calls_json = [tc.to_dict() for tc in tool_calls]
          # Convert to JSON string
          json_string = json.dumps(tool_calls_json, indent=2)
          # Yield the JSON string
          yield json_string
        else:
          # Otherwise, return the content of the first choice
          text = (chunk.choices[0].delta.content
                  if (chunk and chunk.choices[0].delta.content) is not None else '')
          yield text

  @ModelClass.method
  def chat(self,
           messages: List[dict],
           tools: List[dict] = None,
            tool_choice: str = Param(default=None, description="The tool choice for the model. If set to 'auto', Controls which (if any) tool is called by the model.",),
            max_tokens: int = Param(default=512, description="The maximum number of tokens to generate. Shorter token lengths will provide faster performance.", ),
            temperature: float = Param(default=0.7, description="A decimal number that determines the degree of randomness in the response", ),
            top_p: float = Param(default=0.8, description="An alternative to sampling with temperature, where the model considers the results of the tokens with top_p probability mass.", )
           ) -> Iterator[dict]:
    """Chat with the model."""
    if tools is None and tool_choice is None:
      tool_choice = "none"
    elif tools is not None and tool_choice is None:
      # If tools are provided, set tool_choice to "auto"
      # This will allow the model to choose the tool automatically
      # based on the input message
      # and the available tools
      tool_choice = "auto"
    for chunk in self.client.chat(
        messages=messages,
        tools=tools,
        tool_choice=tool_choice,
        max_tokens=max_tokens,
        temperature=temperature,
        top_p=top_p,
        stream=True):
      yield chunk.to_dict()
