model:
  id: "vllm-lmcache-kvcache-offloading-llama-3_2-1b"
  user_id: "luv_2261"
  app_id: "test-upload"
  model_type_id: "text-to-text"

build_info:
  python_version: '3.12'

inference_compute_info:
  cpu_limit: '3'
  cpu_memory: 14Gi
  num_accelerators: 1
  accelerator_type: ["NVIDIA-*"]
  accelerator_memory: 19Gi

checkpoints:
  type: "huggingface"
  repo_id: "unsloth/Llama-3.2-1B-Instruct"
  when: "runtime"
