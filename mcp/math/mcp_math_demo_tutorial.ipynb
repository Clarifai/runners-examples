{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MCP Math Server with Clarifai\n\nThis notebook demonstrates how to integrate an MCP server with an LLM on Clarifai.\n\n## What is MCP and How Does it Work with LLMs?\n\nMCP (Model Context Protocol) servers provide tools that LLMs can call to perform specific tasks. This notebook shows the complete integration workflow:\n\n1. **Deploy MCP Server**: Your MCP server hosts tools (like math operations) on Clarifai infrastructure\n2. **Deploy LLM**: An LLM (like GPT) is also deployed and accessible via API\n3. **Connect the Two**: You write client code that:\n   - Sends user queries to the LLM along with available MCP tools\n   - The LLM decides if it needs to use a tool to answer the query\n   - Your code executes the tool call via the MCP server\n   - Results are sent back to the LLM for a final natural language response\n\nThis pattern enables LLMs to dynamically call external tools based on user requests, extending their capabilities beyond text generation.\n\n## Prerequisites\n\n- Clarifai account with PAT (Personal Access Token)\n- MCP server deployed on Clarifai\n- Python 3.11+"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Deploy MCP Server to Clarifai\n\nBefore running this notebook, deploy your MCP server using Clarifai infrastructure.\n\nFollow the Clarifai documentation to upload and deploy an MCP server: https://docs.clarifai.com/compute/agents/mcp/\n\nAfter deployment, verify the MCP server is active on the Clarifai platform."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Install required packages for MCP client and OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clarifai==11.7.5 fastmcp pydantic openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration\n",
    "\n",
    "Set up your Clarifai PAT and model endpoints. Update these values with your deployed model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# TODO: Set your Clarifai PAT\nos.environ[\"CLARIFAI_PAT\"] = \"your_pat_here\"  # Replace with your actual PAT\n\n# LLM model endpoint (GPT-OSS on Clarifai)\nMODEL_NAME = \"https://clarifai.com/openai/chat-completion/models/gpt-oss-120b\"\n\n# TODO: Update with your deployed MCP server information\nUSER_ID = \"your_user_id\"      # Replace with your Clarifai user ID\nAPP_ID = \"your_app_id\"        # Replace with your MCP app ID\nMODEL_ID = \"your_model_id\"    # Replace with your MCP model ID\n\nMCP_SERVER_URL = f\"https://api.clarifai.com/v2/ext/mcp/v1/users/{USER_ID}/apps/{APP_ID}/models/{MODEL_ID}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Import Required Libraries\n",
    "\n",
    "Import all necessary modules for MCP client, OpenAI client, and async operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StreamableHttpTransport\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Initialize OpenAI client pointing to Clarifai\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.environ[\"CLARIFAI_PAT\"], \n",
    "    base_url=\"https://api.clarifai.com/v2/ext/openai/v1\"\n",
    ")\n",
    "\n",
    "# Global variables for MCP client\n",
    "mcp_client = None\n",
    "mcp_tools = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Connect to MCP Server\n",
    "\n",
    "This function establishes a connection to your deployed MCP server and retrieves available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_mcp():\n",
    "    \"\"\"Connect to the math MCP server.\"\"\"\n",
    "    global mcp_client, mcp_tools\n",
    "\n",
    "    transport = StreamableHttpTransport(\n",
    "        url=MCP_SERVER_URL,\n",
    "        headers={\"Authorization\": \"Bearer \" + os.environ[\"CLARIFAI_PAT\"]}\n",
    "    )\n",
    "\n",
    "    mcp_client = Client(transport)\n",
    "    await mcp_client.__aenter__()\n",
    "\n",
    "    # Get available tools from MCP server\n",
    "    tools_result = await mcp_client.list_tools()\n",
    "    mcp_tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema,\n",
    "            },\n",
    "        }\n",
    "        for tool in tools_result\n",
    "    ]\n",
    "\n",
    "    return mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute MCP Tool Calls\n",
    "\n",
    "This function handles calling MCP tools when the LLM decides to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_mcp_tool(tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "    \"\"\"Execute an MCP tool call.\"\"\"\n",
    "    try:\n",
    "        result = await mcp_client.call_tool(tool_name, arguments)\n",
    "        return result.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error executing {tool_name}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test with Math Queries\n",
    "\n",
    "This function demonstrates the full workflow:\n",
    "1. Send query to LLM with available MCP tools\n",
    "2. LLM decides if it needs to use a tool\n",
    "3. Parse LLM response and execute tool calls via MCP\n",
    "4. Send tool results back to LLM for final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def test_math_tools():\n    \"\"\"\n    Test math tools with simple queries.\n    \n    This function demonstrates the full LLM + MCP integration workflow:\n    1. Send a user query to the LLM with available MCP tools in context\n    2. The LLM analyzes the query and decides whether to use a tool\n    3. If a tool is needed, extract the tool call details and execute via MCP\n    4. Send tool results back to the LLM to generate a natural language response\n    \"\"\"\n\n    test_queries = [\n        \"What is 15.5 + 23.2?\",\n        \"Calculate 100 divided by 8\"\n    ]\n\n    for query in test_queries:\n        print(f\"\\nQuery: {query}\")\n\n        # Step 1: Send query to LLM with MCP tools available\n        # The LLM receives both the user question AND the list of available tools\n        # It can now decide: \"Do I need a tool to answer this, or can I answer directly?\"\n        response = await openai_client.chat.completions.create(\n            model=MODEL_NAME,\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant with access to math tools. Use the appropriate tools when needed.\"\n                },\n                {\"role\": \"user\", \"content\": query}\n            ],\n            tools=mcp_tools,  # This tells the LLM what tools are available\n            tool_choice=\"auto\",  # Let the LLM decide if it needs to use a tool\n            temperature=0.1,\n            max_tokens=500\n        )\n\n        message = response.choices[0].message\n\n        # Step 2: Check if LLM wants to use tools\n        # The LLM's response includes tool_calls if it decided a tool is needed\n        if message.tool_calls:\n            print(f\"Tool calls: {len(message.tool_calls)}\")\n\n            # Step 3: Execute each tool call via MCP\n            # The LLM has specified which tool to call and with what arguments\n            # Now we execute those tool calls using our MCP client\n            tool_responses = []\n            for tool_call in message.tool_calls:\n                # Extract the arguments the LLM wants to pass to the tool\n                args = json.loads(tool_call.function.arguments)\n                \n                # Call the MCP server to execute the tool\n                result = await execute_mcp_tool(tool_call.function.name, args)\n                tool_responses.append(result)\n                print(f\"Tool {tool_call.function.name}: {result}\")\n\n            # Step 4: Send tool results back to LLM for final response\n            # Now the LLM has the tool execution results and can generate a natural answer\n            follow_up_messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a helpful assistant with access to math tools. Use the appropriate tools when needed.\"\n                },\n                {\"role\": \"user\", \"content\": query},\n                message  # Include the LLM's original response with tool calls\n            ]\n\n            # Add the tool results to the conversation\n            for tool_call, tool_result in zip(message.tool_calls, tool_responses):\n                follow_up_messages.append({\n                    \"role\": \"tool\",\n                    \"tool_call_id\": tool_call.id,\n                    \"content\": tool_result\n                })\n\n            # Send everything back to the LLM to generate a final natural language answer\n            final_response = await openai_client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=follow_up_messages,\n                temperature=0.1,\n                max_tokens=500\n            )\n\n            print(f\"Final answer: {final_response.choices[0].message.content}\")\n        else:\n            # The LLM decided it doesn't need tools and answered directly\n            print(f\"Direct answer: {message.content}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup Function\n",
    "\n",
    "Clean up the MCP connection when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleanup():\n",
    "    \"\"\"Clean up MCP connection.\"\"\"\n",
    "    global mcp_client\n",
    "    if mcp_client:\n",
    "        await mcp_client.__aexit__(None, None, None)\n",
    "        mcp_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run the Demo\n",
    "\n",
    "Execute the complete workflow:\n",
    "1. Connect to the deployed MCP server\n",
    "2. Test with math queries\n",
    "3. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"Testing GPT-OSS with Math MCP Tools\")\n",
    "\n",
    "    try:\n",
    "        # Connect to MCP server\n",
    "        await connect_to_mcp()\n",
    "        print(f\"Connected to math MCP server with {len(mcp_tools)} tools\")\n",
    "\n",
    "        # Test the tools\n",
    "        await test_math_tools()\n",
    "\n",
    "    finally:\n",
    "        await cleanup()\n",
    "\n",
    "# Run the async main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the three-part integration:\n",
    "\n",
    "1. **MCP Connection**: Establish connection to deployed MCP server and retrieve available tools\n",
    "2. **LLM Query**: Send test queries to LLM with MCP tools in context so LLM understands what is available\n",
    "3. **Tool Execution**: Parse LLM response, execute tool calls via MCP client, and return results to LLM\n",
    "\n",
    "The workflow enables LLMs to dynamically call tools hosted on your MCP server based on user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}