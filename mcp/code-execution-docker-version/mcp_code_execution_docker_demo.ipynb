{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MCP Python Code Execution with Docker on Clarifai Local Runner\n\nThis notebook demonstrates how to integrate an MCP server with Docker-based Python code execution capabilities running on a Clarifai Local Runner with an LLM.\n\n## What is MCP and How Does it Work with LLMs?\n\nMCP (Model Context Protocol) servers provide tools that LLMs can call to perform specific tasks. This notebook shows the complete integration workflow:\n\n1. **Deploy MCP Server on Local Runner**: Your MCP server hosts tools (like Python code execution in Docker containers) on your local machine using Clarifai Local Runner\n2. **Deploy LLM**: An LLM (like GPT) is deployed and accessible via API on Clarifai\n3. **Connect the Two**: You write client code that:\n   - Sends user queries to the LLM along with available MCP tools\n   - The LLM decides if it needs to use a tool to answer the query\n   - Your code executes the tool call via the MCP server running locally\n   - Results are sent back to the LLM for a final natural language response\n\n## Local Runner vs Cloud Deployment\n\nThis Docker-based version runs on your local machine using Clarifai Local Runner, which:\n- **Runs locally**: MCP server runs on your machine with Docker\n- **Uses Docker**: Provides isolated Python code execution in containers\n- **Better security**: Code runs in isolated containers on your infrastructure\n- **Local resources**: Uses your local machine's compute and Docker daemon\n\n## Prerequisites\n\n- Clarifai account with PAT (Personal Access Token)\n- Docker installed and running on your local machine\n- Clarifai Local Runner set up\n- MCP server with Docker-based code execution deployed via Local Runner\n- Python 3.11+"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Deploy MCP Server with Local Runner\n\nThis MCP server requires deployment using Clarifai Local Runner on your local machine.\n\n### Setup Steps:\n\n1. **Install Docker**: Ensure Docker is installed and running on your local machine\n2. **Set up Local Runner**: Follow the Clarifai Local Runner documentation: https://docs.clarifai.com/compute/local-runners/\n3. **Deploy MCP Server**: Use the command:\n   ```bash\n   clarifai model upload local-runner\n   ```\n   This deploys the MCP server to run on your local machine via Local Runner\n\n4. **Verify Deployment**: Ensure the MCP server is active and accessible locally\n\nThe Local Runner approach allows the MCP server to access your local Docker daemon for isolated code execution."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Install required packages for MCP client and OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clarifai==11.7.5 fastmcp pydantic openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 3: Configuration\n\nSet up your Clarifai PAT and model endpoints.\n\nNote: The MCP server runs locally via Local Runner, but you still connect to it through Clarifai's MCP API endpoint."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# TODO: Set your Clarifai PAT\n",
    "os.environ[\"CLARIFAI_PAT\"] = \"your_pat_here\"  # Replace with your actual PAT\n",
    "\n",
    "# LLM model endpoint (GPT-OSS on Clarifai)\n",
    "MODEL_NAME = \"https://clarifai.com/openai/chat-completion/models/gpt-oss-120b\"\n",
    "\n",
    "# TODO: Update with your deployed MCP server information\n",
    "USER_ID = \"your_user_id\"      # Replace with your Clarifai user ID\n",
    "APP_ID = \"your_app_id\"        # Replace with your MCP app ID\n",
    "MODEL_ID = \"your_model_id\"    # Replace with your MCP model ID\n",
    "\n",
    "MCP_SERVER_URL = f\"https://api.clarifai.com/v2/ext/mcp/v1/users/{USER_ID}/apps/{APP_ID}/models/{MODEL_ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Import Required Libraries\n",
    "\n",
    "Import all necessary modules for MCP client, OpenAI client, and async operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StreamableHttpTransport\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Initialize OpenAI client pointing to Clarifai\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.environ[\"CLARIFAI_PAT\"], \n",
    "    base_url=\"https://api.clarifai.com/v2/ext/openai/v1\"\n",
    ")\n",
    "\n",
    "# Global variables for MCP client\n",
    "mcp_client = None\n",
    "mcp_tools = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Connect to MCP Server\n\nThis function establishes a connection to your MCP server running on Local Runner.\n\nThe connection goes through Clarifai's API, but the MCP server itself runs locally with access to your Docker daemon."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_mcp():\n",
    "    \"\"\"Connect to the Python code execution MCP server.\"\"\"\n",
    "    global mcp_client, mcp_tools\n",
    "\n",
    "    transport = StreamableHttpTransport(\n",
    "        url=MCP_SERVER_URL,\n",
    "        headers={\"Authorization\": \"Bearer \" + os.environ[\"CLARIFAI_PAT\"]}\n",
    "    )\n",
    "\n",
    "    mcp_client = Client(transport)\n",
    "    await mcp_client.__aenter__()\n",
    "\n",
    "    # Get available tools from MCP server\n",
    "    tools_result = await mcp_client.list_tools()\n",
    "    mcp_tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema,\n",
    "            },\n",
    "        }\n",
    "        for tool in tools_result\n",
    "    ]\n",
    "\n",
    "    return mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute MCP Tool Calls\n",
    "\n",
    "This function handles calling MCP tools when the LLM decides to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_mcp_tool(tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "    \"\"\"Execute an MCP tool call.\"\"\"\n",
    "    try:\n",
    "        result = await mcp_client.call_tool(tool_name, arguments)\n",
    "        return result.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error executing {tool_name}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test with Docker-Based Code Execution Queries\n",
    "\n",
    "This function demonstrates the full workflow:\n",
    "1. Send query to LLM with available MCP tools\n",
    "2. LLM decides if it needs to use a tool and generates code\n",
    "3. Parse LLM response and execute tool calls via MCP (in Docker containers)\n",
    "4. Send tool results back to LLM for final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_queries():\n",
    "    \"\"\"\n",
    "    Test with code execution queries.\n",
    "    \n",
    "    This function demonstrates the full LLM + MCP integration workflow:\n",
    "    1. Send a user query to the LLM with available MCP tools in context\n",
    "    2. The LLM analyzes the query and decides whether to use a tool\n",
    "    3. If a tool is needed, extract the tool call details (including generated code) and execute via MCP in Docker\n",
    "    4. Send tool results back to the LLM to generate a natural language response\n",
    "    \"\"\"\n",
    "\n",
    "    test_queries = [\n",
    "        \"Use time package in python to print the current time\",\n",
    "        \"Use the numpy package to perform a matrix multiplication and print the result\"\n",
    "    ]\n",
    "\n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Query {i}: {query}\")\n",
    "        print('='*50)\n",
    "\n",
    "        try:\n",
    "            # Step 1: Send query to LLM with MCP tools available\n",
    "            # The LLM receives both the user question AND the list of available tools\n",
    "            # It can now decide: \"Do I need to write and execute code for this?\"\n",
    "            response = await openai_client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a Python assistant with code execution tools.\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": query}\n",
    "                ],\n",
    "                tools=mcp_tools,  # This tells the LLM what tools are available\n",
    "                tool_choice=\"auto\",  # Let the LLM decide if it needs to use a tool\n",
    "                temperature=0.1,\n",
    "                max_tokens=500\n",
    "            )\n",
    "\n",
    "            message = response.choices[0].message\n",
    "            print(f\"Assistant: {message.content}\")\n",
    "\n",
    "            # Step 2: Check if LLM wants to use tools\n",
    "            # The LLM's response includes tool_calls if it decided a tool is needed\n",
    "            if message.tool_calls:\n",
    "                print(f\"\\nTool calls: {len(message.tool_calls)}\")\n",
    "\n",
    "                # Step 3: Execute each tool call via MCP\n",
    "                # The LLM has generated Python code and specified which tool to use\n",
    "                # Now we execute those tool calls using our MCP client (code runs in Docker)\n",
    "                tool_responses = []\n",
    "                for tool_call in message.tool_calls:\n",
    "                    print(f\"\\n--- {tool_call.function.name} ---\")\n",
    "                    \n",
    "                    # Extract the arguments the LLM wants to pass to the tool\n",
    "                    args = json.loads(tool_call.function.arguments)\n",
    "\n",
    "                    # Display what the LLM generated\n",
    "                    if \"code\" in args:\n",
    "                        print(f\"Code: {args['code']}\")\n",
    "                    if \"packages\" in args:\n",
    "                        print(f\"Packages: {args['packages']}\")\n",
    "\n",
    "                    # Call the MCP server to execute the Python code in a Docker container\n",
    "                    result = await execute_mcp_tool(tool_call.function.name, args)\n",
    "                    tool_responses.append(result)\n",
    "                    print(f\"Result:\\n{result}\")\n",
    "\n",
    "                # Step 4: Send tool results back to LLM for final response\n",
    "                # Now the LLM has the code execution results and can generate a natural answer\n",
    "                follow_up_messages = [\n",
    "                    {\"role\": \"system\", \"content\": \"You are a Python assistant with code execution tools.\"},\n",
    "                    {\"role\": \"user\", \"content\": query},\n",
    "                    message  # Include the LLM's original response with tool calls\n",
    "                ]\n",
    "\n",
    "                # Add the tool results to the conversation\n",
    "                for tool_call, tool_result in zip(message.tool_calls, tool_responses):\n",
    "                    follow_up_messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tool_call.id,\n",
    "                        \"content\": tool_result\n",
    "                    })\n",
    "\n",
    "                # Send everything back to the LLM to generate a final natural language answer\n",
    "                final_response = await openai_client.chat.completions.create(\n",
    "                    model=MODEL_NAME,\n",
    "                    messages=follow_up_messages,\n",
    "                    temperature=0.1,\n",
    "                    max_tokens=300\n",
    "                )\n",
    "\n",
    "                print(f\"\\nFinal: {final_response.choices[0].message.content}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup Function\n",
    "\n",
    "Clean up the MCP connection when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleanup():\n",
    "    \"\"\"Clean up MCP connection.\"\"\"\n",
    "    global mcp_client\n",
    "    if mcp_client:\n",
    "        await mcp_client.__aexit__(None, None, None)\n",
    "        mcp_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run the Demo\n",
    "\n",
    "Execute the complete workflow:\n",
    "1. Connect to the deployed MCP server\n",
    "2. Test with code execution queries\n",
    "3. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"Testing GPT-OSS with Docker-Based Python Execution Tools\")\n",
    "\n",
    "    try:\n",
    "        # Connect to MCP server\n",
    "        await connect_to_mcp()\n",
    "        print(f\"Connected! Tools: {len(mcp_tools)}\")\n",
    "\n",
    "        # Test the tools\n",
    "        await test_queries()\n",
    "\n",
    "    finally:\n",
    "        await cleanup()\n",
    "\n",
    "# Run the async main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary\n\nThis notebook demonstrates the three-part integration with Docker-based execution on Local Runner:\n\n1. **MCP Connection**: Establish connection to MCP server deployed on Local Runner and retrieve available tools\n2. **LLM Query**: Send test queries to LLM with MCP tools in context so LLM understands what is available and can decide whether to call tools\n3. **Tool Execution**: Parse LLM response, execute tool calls via MCP client running on Local Runner with Docker, and return results to LLM\n\n## Local Runner Benefits:\n\n- **Local Execution**: MCP server runs on your machine via Local Runner\n- **Docker Access**: Direct access to your local Docker daemon for container execution\n- **Isolation**: Each code execution runs in a separate Docker container\n- **Security**: Code runs in isolated containers on your infrastructure\n- **Package Management**: Easy installation of Python packages per execution\n- **Control**: Full control over compute resources and execution environment\n\nThe workflow enables LLMs to dynamically execute Python code through your locally-hosted MCP server in secure Docker environments."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}