{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MCP Python Code Execution Server with Clarifai\n\nThis notebook demonstrates how to integrate an MCP server with Python code execution capabilities with an LLM on Clarifai.\n\n## What is MCP and How Does it Work with LLMs?\n\nMCP (Model Context Protocol) servers provide tools that LLMs can call to perform specific tasks. This notebook shows the complete integration workflow:\n\n1. **Deploy MCP Server**: Your MCP server hosts tools (like Python code execution) on Clarifai infrastructure\n2. **Deploy LLM**: An LLM (like GPT) is also deployed and accessible via API\n3. **Connect the Two**: You write client code that:\n   - Sends user queries to the LLM along with available MCP tools\n   - The LLM decides if it needs to use a tool to answer the query\n   - Your code executes the tool call via the MCP server\n   - Results are sent back to the LLM for a final natural language response\n\nThis pattern enables LLMs to dynamically execute Python code through your MCP server, extending their capabilities to run actual computations.\n\n## Prerequisites\n\n- Clarifai account with PAT (Personal Access Token)\n- MCP server with code execution capabilities deployed on Clarifai\n- Python 3.11+"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 1: Deploy MCP Server to Clarifai\n\nBefore running this notebook, deploy your MCP server using Clarifai infrastructure.\n\nFollow the Clarifai documentation to upload and deploy an MCP server: https://docs.clarifai.com/compute/agents/mcp/\n\nAfter deployment, verify the MCP server is active on the Clarifai platform."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Dependencies\n",
    "\n",
    "Install required packages for MCP client and OpenAI integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clarifai==11.7.5 fastmcp pydantic openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration\n",
    "\n",
    "Set up your Clarifai PAT and model endpoints. Update these values with your deployed model information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# TODO: Set your Clarifai PAT\nos.environ[\"CLARIFAI_PAT\"] = \"your_pat_here\"  # Replace with your actual PAT\n\n# LLM model endpoint (GPT-OSS on Clarifai)\nMODEL_NAME = \"https://clarifai.com/openai/chat-completion/models/gpt-oss-120b\"\n\n# TODO: Update with your deployed MCP server information\nUSER_ID = \"your_user_id\"      # Replace with your Clarifai user ID\nAPP_ID = \"your_app_id\"        # Replace with your MCP app ID\nMODEL_ID = \"your_model_id\"    # Replace with your MCP model ID\n\nMCP_SERVER_URL = f\"https://api.clarifai.com/v2/ext/mcp/v1/users/{USER_ID}/apps/{APP_ID}/models/{MODEL_ID}\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Import Required Libraries\n",
    "\n",
    "Import all necessary modules for MCP client, OpenAI client, and async operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "from typing import Any, Dict\n",
    "\n",
    "from fastmcp import Client\n",
    "from fastmcp.client.transports import StreamableHttpTransport\n",
    "from openai import AsyncOpenAI\n",
    "\n",
    "# Initialize OpenAI client pointing to Clarifai\n",
    "openai_client = AsyncOpenAI(\n",
    "    api_key=os.environ[\"CLARIFAI_PAT\"], \n",
    "    base_url=\"https://api.clarifai.com/v2/ext/openai/v1\"\n",
    ")\n",
    "\n",
    "# Global variables for MCP client\n",
    "mcp_client = None\n",
    "mcp_tools = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Connect to MCP Server\n",
    "\n",
    "This function establishes a connection to your deployed MCP server and retrieves available tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def connect_to_mcp():\n",
    "    \"\"\"Connect to the Python code execution MCP server.\"\"\"\n",
    "    global mcp_client, mcp_tools\n",
    "\n",
    "    transport = StreamableHttpTransport(\n",
    "        url=MCP_SERVER_URL,\n",
    "        headers={\"Authorization\": \"Bearer \" + os.environ[\"CLARIFAI_PAT\"]}\n",
    "    )\n",
    "\n",
    "    mcp_client = Client(transport)\n",
    "    await mcp_client.__aenter__()\n",
    "\n",
    "    # Get available tools from MCP server\n",
    "    tools_result = await mcp_client.list_tools()\n",
    "    mcp_tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": tool.name,\n",
    "                \"description\": tool.description,\n",
    "                \"parameters\": tool.inputSchema,\n",
    "            },\n",
    "        }\n",
    "        for tool in tools_result\n",
    "    ]\n",
    "\n",
    "    return mcp_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Execute MCP Tool Calls\n",
    "\n",
    "This function handles calling MCP tools when the LLM decides to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_mcp_tool(tool_name: str, arguments: Dict[str, Any]) -> str:\n",
    "    \"\"\"Execute an MCP tool call.\"\"\"\n",
    "    try:\n",
    "        result = await mcp_client.call_tool(tool_name, arguments)\n",
    "        return result.content[0].text\n",
    "    except Exception as e:\n",
    "        return f\"Error executing {tool_name}: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Test with Code Execution Queries\n",
    "\n",
    "This function demonstrates the full workflow:\n",
    "1. Send query to LLM with available MCP tools\n",
    "2. LLM decides if it needs to use a tool and generates code\n",
    "3. Parse LLM response and execute tool calls via MCP\n",
    "4. Send tool results back to LLM for final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "async def test_queries():\n    \"\"\"\n    Test with code execution queries.\n    \n    This function demonstrates the full LLM + MCP integration workflow:\n    1. Send a user query to the LLM with available MCP tools in context\n    2. The LLM analyzes the query and decides whether to use a tool\n    3. If a tool is needed, extract the tool call details (including generated code) and execute via MCP\n    4. Send tool results back to the LLM to generate a natural language response\n    \"\"\"\n\n    test_queries = [\n        \"Use time package in python to print the current time\",\n        \"Use the numpy python package to perform a matrix multiplication of [[1, 2], [3, 4]] and [[5, 6], [7, 8]] and print the result\"\n    ]\n\n    for i, query in enumerate(test_queries, 1):\n        print(f\"\\n{'='*50}\")\n        print(f\"Query {i}: {query}\")\n        print('='*50)\n\n        try:\n            # Step 1: Send query to LLM with MCP tools available\n            # The LLM receives both the user question AND the list of available tools\n            # It can now decide: \"Do I need to write and execute code for this?\"\n            response = await openai_client.chat.completions.create(\n                model=MODEL_NAME,\n                messages=[\n                    {\n                        \"role\": \"system\",\n                        \"content\": \"You are a Python assistant with code execution tools.\"\n                    },\n                    {\"role\": \"user\", \"content\": query}\n                ],\n                tools=mcp_tools,  # This tells the LLM what tools are available\n                tool_choice=\"auto\",  # Let the LLM decide if it needs to use a tool\n                temperature=0.1,\n                max_tokens=500\n            )\n\n            message = response.choices[0].message\n            print(f\"Assistant: {message.content}\")\n\n            # Step 2: Check if LLM wants to use tools\n            # The LLM's response includes tool_calls if it decided a tool is needed\n            if message.tool_calls:\n                print(f\"\\nTool calls: {len(message.tool_calls)}\")\n\n                # Step 3: Execute each tool call via MCP\n                # The LLM has generated Python code and specified which tool to use\n                # Now we execute those tool calls using our MCP client\n                tool_responses = []\n                for tool_call in message.tool_calls:\n                    print(f\"\\n--- {tool_call.function.name} ---\")\n                    \n                    # Extract the arguments the LLM wants to pass to the tool\n                    args = json.loads(tool_call.function.arguments)\n\n                    # Display what the LLM generated\n                    if \"code\" in args:\n                        print(f\"Code: {args['code']}\")\n                    if \"packages\" in args:\n                        print(f\"Packages: {args['packages']}\")\n\n                    # Call the MCP server to execute the Python code\n                    result = await execute_mcp_tool(tool_call.function.name, args)\n                    tool_responses.append(result)\n                    print(f\"Result:\\n{result}\")\n\n                # Step 4: Send tool results back to LLM for final response\n                # Now the LLM has the code execution results and can generate a natural answer\n                follow_up_messages = [\n                    {\"role\": \"system\", \"content\": \"You are a Python assistant with code execution tools.\"},\n                    {\"role\": \"user\", \"content\": query},\n                    message  # Include the LLM's original response with tool calls\n                ]\n\n                # Add the tool results to the conversation\n                for tool_call, tool_result in zip(message.tool_calls, tool_responses):\n                    follow_up_messages.append({\n                        \"role\": \"tool\",\n                        \"tool_call_id\": tool_call.id,\n                        \"content\": tool_result\n                    })\n\n                # Send everything back to the LLM to generate a final natural language answer\n                final_response = await openai_client.chat.completions.create(\n                    model=MODEL_NAME,\n                    messages=follow_up_messages,\n                    temperature=0.1,\n                    max_tokens=300\n                )\n\n                print(f\"\\nFinal: {final_response.choices[0].message.content}\")\n\n        except Exception as e:\n            print(f\"Error: {str(e)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Cleanup Function\n",
    "\n",
    "Clean up the MCP connection when done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def cleanup():\n",
    "    \"\"\"Clean up MCP connection.\"\"\"\n",
    "    global mcp_client\n",
    "    if mcp_client:\n",
    "        await mcp_client.__aexit__(None, None, None)\n",
    "        mcp_client = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Run the Demo\n",
    "\n",
    "Execute the complete workflow:\n",
    "1. Connect to the deployed MCP server\n",
    "2. Test with code execution queries\n",
    "3. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"Testing GPT-OSS with Python Execution Tools\")\n",
    "\n",
    "    try:\n",
    "        # Connect to MCP server\n",
    "        await connect_to_mcp()\n",
    "        print(f\"Connected! Tools: {len(mcp_tools)}\")\n",
    "\n",
    "        # Test the tools\n",
    "        await test_queries()\n",
    "\n",
    "    finally:\n",
    "        await cleanup()\n",
    "\n",
    "# Run the async main function\n",
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the three-part integration:\n",
    "\n",
    "1. **MCP Connection**: Establish connection to deployed MCP server and retrieve available tools\n",
    "2. **LLM Query**: Send test queries to LLM with MCP tools in context so LLM understands what is available and can decide whether to call tools\n",
    "3. **Tool Execution**: Parse LLM response, execute tool calls via MCP client, and return results to LLM\n",
    "\n",
    "The workflow enables LLMs to dynamically execute Python code through tools hosted on your MCP server based on user queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}